{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd29eaa7",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 3: DETECTION OF SPAM MAILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8281d",
   "metadata": {},
   "source": [
    "In this assignment, it is asked to implement naive bayes algorithm from the scratch and to apply this algorithm for given ham/spam email dataset. Our aim is to predict whether an e-mail is a spam or not using the naive bayes model. This report shows steps we have followed, code implementations and analysis of results for naive bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f65d68",
   "metadata": {},
   "source": [
    "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem. The Naive Bayes algorithm is quick and simple to implement, but the necessity that predictors be independent is its major drawback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ffaa6",
   "metadata": {},
   "source": [
    "## PART 1: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493d8a2",
   "metadata": {},
   "source": [
    "Machine learning algorithms classify data using statistical models. In the instance of spam detection, a trained machine learning model must be able to tell if the sequence of words in an email is more similar to spam emails or not. Different machine learning methods can detect spam, in this assignment we use Naive Bayes. The \"Bayes' theorem,\" which describes the likelihood of an event dependent on prior knowledge, is the foundation of nave Bayes. The problem is that our characteristics aren't always self-contained. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aabada",
   "metadata": {},
   "source": [
    "Consider the words \"not\" and \"interesting\", in this situation, depending on where they appear in the message, the meaning can be radically different. Despite the fact that feature independence in text data is difficult to achieve, the Naive Bayes classifier has proven to be effective in natural language processing applications when correctly configured. In the following sections, it is going to be tested and interpreted whether Naive Bayes is succesful on email spam detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d2a23",
   "metadata": {},
   "source": [
    "Naive Bayes, like other machine learning algorithms, does not understand linguistic context and instead relies on statistical relationships between words to determine whether a piece of text belongs to a specific class. This means that a naive Bayes spam detector, for example, can be fooled into missing a spam email if the sender simply adds some non-spam words at the end of the message or replaces spammy terms with closely related words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27432dc4",
   "metadata": {},
   "source": [
    "On the other hand, many spam e-mails contain a large amount of \"spammy\" terms such as \"free,\" \"money,\" \"product,\" and so on. This knowledge may aid us in making better decisions when it comes to the design of the spam detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c838d5",
   "metadata": {},
   "source": [
    "Therefore, detecting e-mails by analizing its words is a feasible approach to solve this problem. Generally, manipulative, needy, cheap, shady and fat-fetched words such as \"exclusive deal\", \"free, \"limited time\", \"offer expires\", \"urgent\", \"best price\", \"bargain\", \"great offer\", \"money\" are well-known as spammy. On the other hand, classifying e-mails based on only words can be problematic due to the deficit of context interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311428c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24abba36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subject: great nnews  hello , welcome to medzo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subject: here ' s a hot play in motion  homela...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Subject: undeliverable : home based business f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1\n",
       "5  Subject: great nnews  hello , welcome to medzo...     1\n",
       "6  Subject: here ' s a hot play in motion  homela...     1\n",
       "7  Subject: save your money buy getting this thin...     1\n",
       "8  Subject: undeliverable : home based business f...     1\n",
       "9  Subject: save your money buy getting this thin...     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"emails.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e45b166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (5728, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3632a627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4360\n",
       "1    1368\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"spam\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97cf07c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='spam', ylabel='count'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x=\"spam\", data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427351d",
   "metadata": {},
   "source": [
    "As can be seen above, the given data has two columns which are \"text\" and \"spam\". Spam=1 means that this e-mail is actually spam. Thus, our data has 4360 ham and 1368 spam mails which makes total 5728 e-mail. It can be seen that there is an imbalance on the number of ham and spam e-mails in given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5984757",
   "metadata": {},
   "source": [
    "In order to be able to interpret the data, it is needed to compute the frequencies of words and create a dictionary which holds unique words in the given data with their frequencies. After reading the data file and storing it into a pandas dataframe, the data is splitted into train and test with shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "154566d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(vectorizer, xtrain, ytrain):\n",
    "    y = vectorizer.fit_transform(xtrain)\n",
    "    doc_array = y.toarray()\n",
    "\n",
    "    frequency_matrix = pd.DataFrame(data=doc_array, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    words = list(frequency_matrix.columns.values)\n",
    "    freq = {}\n",
    "\n",
    "    for i, j in frequency_matrix.iterrows():\n",
    "        create_dictionary(freq, words, j.tolist(), ytrain[i])\n",
    "    return freq, frequency_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97acefa",
   "metadata": {},
   "source": [
    "In this function, the vectorizer is supplied with the training data and frequency matrix is created. After that each row in frequency matrix is sent to \"create_dictionary\" function to calculate frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "921fbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(freq, words, row, is_spam):\n",
    "    i = 0\n",
    "    for item in row:\n",
    "        if item != 0:\n",
    "            if words[i] in freq:\n",
    "                arr = freq[words[i]]\n",
    "                arr[is_spam] += 1\n",
    "                freq[words[i]] = arr\n",
    "\n",
    "            else:\n",
    "                arr = [0, 0]\n",
    "                arr[is_spam] = 1\n",
    "                freq[words[i]] = arr\n",
    "        i += 1\n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8041238",
   "metadata": {},
   "source": [
    "This function returns a dictioanry which includes each unique word as key and its frequencies on spam and ham mails as a list in the value. For instance, 'subject', [3498, 1084] means that the word \"subject\" is mentioned in 3498 ham mails and 1084 spam mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58f7f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_unigram(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), token_pattern=r'\\b[a-zA-Z]\\w+\\b',\n",
    "                                 lowercase=True, min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac16bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_unigram_without_stopwords(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), token_pattern=r'\\b[a-zA-Z]\\w+\\b',\n",
    "                                 lowercase=True, stop_words=ENGLISH_STOP_WORDS, min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2d7c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_bigram(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2), token_pattern=r'\\b[a-zA-Z]\\w+\\b',\n",
    "                                 lowercase=True, min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7fc28596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_bigram_without_stopwords(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2), token_pattern=r'\\b[a-zA-Z]\\w+\\b',\n",
    "                                 lowercase=True, stop_words=ENGLISH_STOP_WORDS, min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2b35342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_unigram_bigram(xtrain, ytrain):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2), token_pattern=r'\\b[a-zA-Z]\\w+\\b', lowercase=True)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186195d",
   "metadata": {},
   "source": [
    "The functions above create different vectorizers according to whether words are tokenized as unigram or bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9123c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "df = pd.read_csv('emails.csv')\n",
    "x = df.text.values\n",
    "y = df.spam.values\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "baab6b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------UNIGRAM MOST FREQUENT WORDS----------------------------\n",
      "[('subject', [3498, 1084]), ('to', [3257, 920]), ('the', [3212, 853])]\n",
      "-----------------UNIGRAM (WITHOUT STOPWORDS) MOST FREQUENT WORDS-----------------\n",
      "[('subject', [3498, 1084]), ('vince', [2234, 1]), ('enron', [2062, 0])]\n"
     ]
    }
   ],
   "source": [
    "unigram_freq, freq_matrix = get_frequencies_unigram(xtrain, ytrain, 1, 1.0)\n",
    "sorted_unigram = sorted(unigram_freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "unigram_freq_no_stopwords, freq_matrix_no_stopwords = get_frequencies_unigram_without_stopwords(xtrain, ytrain, 1, 1.0)\n",
    "sorted_unigram_no_stopwords = sorted(unigram_freq_no_stopwords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"--------------------------UNIGRAM MOST FREQUENT WORDS----------------------------\")\n",
    "print(list(sorted_unigram)[:3])\n",
    "print(\"-----------------UNIGRAM (WITHOUT STOPWORDS) MOST FREQUENT WORDS-----------------\")\n",
    "print(list(sorted_unigram_no_stopwords)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad7e83",
   "metadata": {},
   "source": [
    "It can be seen that the word \"subject\" is the most frequent word in both ham and spam mails but it does not help our algorithm to classify better since it is not rare and cannot distinguish spam e-mails from ham e-mails. When we include stopwords, \"to\" and \"the\" are the next most frequent words in both spam and ham e-mails. However, they don't contribute any useful information neither, since they are frequently written in both ham and spam e-mails. On the other hand, when stopwords are removed, the most frequent words (in ham e-mails) after \"subject\" are \"vince\" and \"enron\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00ff4bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaenerfax</th>\n",
       "      <th>aadedeji</th>\n",
       "      <th>aagrawal</th>\n",
       "      <th>aal</th>\n",
       "      <th>aaldous</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aall</th>\n",
       "      <th>aanalysis</th>\n",
       "      <th>...</th>\n",
       "      <th>zwrocic</th>\n",
       "      <th>zwwyw</th>\n",
       "      <th>zwzm</th>\n",
       "      <th>zxghlajf</th>\n",
       "      <th>zyc</th>\n",
       "      <th>zygoma</th>\n",
       "      <th>zymg</th>\n",
       "      <th>zzn</th>\n",
       "      <th>zzncacst</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaaenerfax  aadedeji  aagrawal  aal  aaldous  aaliyah  aall  \\\n",
       "0   0    0           0         0         0    0        0        0     0   \n",
       "1   0    0           0         0         0    0        0        0     0   \n",
       "2   0    0           0         0         0    0        0        0     0   \n",
       "3   0    0           0         0         0    0        0        0     0   \n",
       "4   0    0           0         0         0    0        0        0     0   \n",
       "5   0    0           0         0         0    0        0        0     0   \n",
       "6   0    0           0         0         0    0        0        0     0   \n",
       "7   0    0           0         0         0    0        0        0     0   \n",
       "8   0    0           0         0         0    0        0        0     0   \n",
       "9   0    0           0         0         0    0        0        0     0   \n",
       "\n",
       "   aanalysis  ...  zwrocic  zwwyw  zwzm  zxghlajf  zyc  zygoma  zymg  zzn  \\\n",
       "0          0  ...        0      0     0         0    0       0     0    0   \n",
       "1          0  ...        0      0     0         0    0       0     0    0   \n",
       "2          0  ...        0      0     0         0    0       0     0    0   \n",
       "3          0  ...        0      0     0         0    0       0     0    0   \n",
       "4          0  ...        0      0     0         0    0       0     0    0   \n",
       "5          0  ...        0      0     0         0    0       0     0    0   \n",
       "6          0  ...        0      0     0         0    0       0     0    0   \n",
       "7          0  ...        0      0     0         0    0       0     0    0   \n",
       "8          0  ...        0      0     0         0    0       0     0    0   \n",
       "9          0  ...        0      0     0         0    0       0     0    0   \n",
       "\n",
       "   zzncacst  zzzz  \n",
       "0         0     0  \n",
       "1         0     0  \n",
       "2         0     0  \n",
       "3         0     0  \n",
       "4         0     0  \n",
       "5         0     0  \n",
       "6         0     0  \n",
       "7         0     0  \n",
       "8         0     0  \n",
       "9         0     0  \n",
       "\n",
       "[10 rows x 30259 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_matrix.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1ccbbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaenerfax</th>\n",
       "      <th>aadedeji</th>\n",
       "      <th>aagrawal</th>\n",
       "      <th>aal</th>\n",
       "      <th>aaldous</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aall</th>\n",
       "      <th>aanalysis</th>\n",
       "      <th>...</th>\n",
       "      <th>zwrocic</th>\n",
       "      <th>zwwyw</th>\n",
       "      <th>zwzm</th>\n",
       "      <th>zxghlajf</th>\n",
       "      <th>zyc</th>\n",
       "      <th>zygoma</th>\n",
       "      <th>zymg</th>\n",
       "      <th>zzn</th>\n",
       "      <th>zzncacst</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29954 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaaenerfax  aadedeji  aagrawal  aal  aaldous  aaliyah  aall  \\\n",
       "0   0    0           0         0         0    0        0        0     0   \n",
       "1   0    0           0         0         0    0        0        0     0   \n",
       "2   0    0           0         0         0    0        0        0     0   \n",
       "3   0    0           0         0         0    0        0        0     0   \n",
       "4   0    0           0         0         0    0        0        0     0   \n",
       "5   0    0           0         0         0    0        0        0     0   \n",
       "6   0    0           0         0         0    0        0        0     0   \n",
       "7   0    0           0         0         0    0        0        0     0   \n",
       "8   0    0           0         0         0    0        0        0     0   \n",
       "9   0    0           0         0         0    0        0        0     0   \n",
       "\n",
       "   aanalysis  ...  zwrocic  zwwyw  zwzm  zxghlajf  zyc  zygoma  zymg  zzn  \\\n",
       "0          0  ...        0      0     0         0    0       0     0    0   \n",
       "1          0  ...        0      0     0         0    0       0     0    0   \n",
       "2          0  ...        0      0     0         0    0       0     0    0   \n",
       "3          0  ...        0      0     0         0    0       0     0    0   \n",
       "4          0  ...        0      0     0         0    0       0     0    0   \n",
       "5          0  ...        0      0     0         0    0       0     0    0   \n",
       "6          0  ...        0      0     0         0    0       0     0    0   \n",
       "7          0  ...        0      0     0         0    0       0     0    0   \n",
       "8          0  ...        0      0     0         0    0       0     0    0   \n",
       "9          0  ...        0      0     0         0    0       0     0    0   \n",
       "\n",
       "   zzncacst  zzzz  \n",
       "0         0     0  \n",
       "1         0     0  \n",
       "2         0     0  \n",
       "3         0     0  \n",
       "4         0     0  \n",
       "5         0     0  \n",
       "6         0     0  \n",
       "7         0     0  \n",
       "8         0     0  \n",
       "9         0     0  \n",
       "\n",
       "[10 rows x 29954 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_matrix_no_stopwords.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5fe29a",
   "metadata": {},
   "source": [
    "The frequency matrices for unigram are shown above. The first matrix has 30259 columns which means there are 30259 unique words, each row shows the frequencies of each word on that particular e-mail. For instance, in the first e-mail of our data, the word \"aa\" is not typed because its value is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc01b68",
   "metadata": {},
   "source": [
    "## PART 2: Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d663c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(ham_count, spam_count, freq, xtest, ngram):\n",
    "    y_pred = []\n",
    "    for test in xtest:\n",
    "        p_spam = math.log2(spam_count / (spam_count + ham_count))\n",
    "        p_ham = math.log2(ham_count / (spam_count + ham_count))\n",
    "\n",
    "        # probability for spam\n",
    "        p_spam += calculate_probability(test, freq, spam_count, 1, ngram)\n",
    "\n",
    "        # probability for spam\n",
    "        p_ham += calculate_probability(test, freq, ham_count, 0, ngram)\n",
    "\n",
    "        if p_spam > p_ham:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef3561",
   "metadata": {},
   "source": [
    "In this function, the probability of being spam or ham are calculated respectively. Then, e-mail is labeled as spam if the probability calculated for spam is greater than probability calculated for ham, vice versa. One thing important here to state that the probabilities are calculated with loagirthm function in order to prevent numerical underflow when calculating multiplicative probabilities. The probabilities are substitued into logarithm function, then they are summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e642b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(test, freq, class_count, is_spam, ngram):\n",
    "    prob = 0\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(ngram, ngram))\n",
    "    y = vectorizer.fit_transform([test])\n",
    "    test = vectorizer.get_feature_names_out()\n",
    "\n",
    "    for word in test:\n",
    "        if word in freq:\n",
    "            prob += laplace_smoothing(freq[word][is_spam], 1, class_count, len(freq))\n",
    "\n",
    "        else:\n",
    "            prob += laplace_smoothing(0, 1, class_count, len(freq))\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94510421",
   "metadata": {},
   "source": [
    "This function calls laplace smoothing for each word in test sample, sums results up, returns the total probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92fe90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(freq, alpha, class_count, total_count):\n",
    "    return math.log2((freq + alpha) / (class_count + total_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393b45f",
   "metadata": {},
   "source": [
    "When calculating the probabilites, we may encounter some words that we havent during training. In this case, the frequency of that word would be 0, which dominates the result of our probability. This problem can be eliminated by using Laplace Smoothing also known as Add One Smoothing when alpha is 1. In this technique, we add 1 to the numerator, and we add k to the denominator. So, in the case that we don’t have a particular ingredient in our training set, the posterior probability comes out to 1 / N + k instead of zero. Plugging in this value into the product does not destroy our ability to make a prediction in the same way that plugging in a zero does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3369fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(ytest, ypred):\n",
    "    tp = tn = fp = fn = 0\n",
    "    for i in range(len(ytest)):\n",
    "        if ytest[i] == ypred[i]:\n",
    "            if ytest[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if ypred[i] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(\"accuracy  : \" + str(accuracy))\n",
    "    print(\"precision : \" + str(precision))\n",
    "    print(\"recall    : \" + str(recall))\n",
    "    print(\"f1_score  : \" + str(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a46dc",
   "metadata": {},
   "source": [
    "In this function, predicted classification and actual classification are compared. Accuracy, recall, precision and f1 score are calculated by computing confusion matrix values like FN, FP, TP and TN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ba223",
   "metadata": {},
   "source": [
    "## PART 3: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86c83f",
   "metadata": {},
   "source": [
    "When you train your machine learning model on the training data set, each term is assigned a weight based on how many times it appears in spam and ham emails. For example, if \"win big money prize\" is one of your features and only appears in spam emails, it will be assigned a higher probability of being spam. If \"important meeting\" is only mentioned in ham emails, then including it in an email increases the likelihood of that email being classified as not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ad10f",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ed62a",
   "metadata": {},
   "source": [
    "Before supplying our data to CountVectorizer it is required to preprocess the data. First of all, lowering the case of e-mails is essential since the words \"HELLO\" and \"hello\" are the same and lowering them is very helpful for reducing the dimensions by shrinking the size of the dictionary. Whitespaces are removed since they do not have any impact on our classifier. Lastly, numbers are discarded since they do not provide any difference to make our algorithm works more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1abd3f2",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783db96",
   "metadata": {},
   "source": [
    "The definition of a stop word varies. A stop word is a word that appears frequently in a corpus. Or, given a context, you can consider any word that is devoid of true meaning. Articles and some verbs are commonly regarded as stop words because they do not assist us in determining the context or true meaning of a sentence. These are words that can be removed without affecting the final model that you are training. However, the removal of stopwords is not necessary in all situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f88b1",
   "metadata": {},
   "source": [
    "Stopwords should be removed if they add no new information to your problem. They are usually unnecessary in classification problems because it is possible to discuss the general idea of a text even if stop words are removed. Thus, we remove the low-level information from our text by removing these words, allowing us to focus on the important information. In other words, the removal of such words has no negative consequences for the model we train for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89c6d8",
   "metadata": {},
   "source": [
    "Removal of stop words definitely reduces the dataset size and thus reduces the training time due to the fewer number of tokens involved in the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb459a",
   "metadata": {},
   "source": [
    "In e-mail classification, the occurence of stopwords are not essential because the context of the mail, whether it is spam or ham, can be inferred from the other words. Furthermore, removal of stopwords shrinks the data and improves the performance of our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa3241",
   "metadata": {},
   "source": [
    "### Tokenizing - Unigram, Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dcd8fd",
   "metadata": {},
   "source": [
    "Text data must be \"tokenized\" before being fed to machine learning algorithms, both for training and for making predictions on new data. Tokenization is the process of dividing text data into smaller pieces. If you divide the preceding data set into single words (also known as unigrams), you'll get the following vocabulary. It's worth noting that I only used each word once. If we use occurence of a single word, it is called unigram. On the other hand, if we use the occurence of two adjacent words, it is called bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce94de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df min_df açıkla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47057d57",
   "metadata": {},
   "source": [
    "## PART 3: Calculation of Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "720ffca0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------UNIGRAM DEFAULT PARAM----------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/kdp92n316wddp9pz1d_9m9400000gn/T/ipykernel_14409/3139537455.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/07/kdp92n316wddp9pz1d_9m9400000gn/T/ipykernel_14409/3139537455.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------UNIGRAM DEFAULT PARAM----------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_frequencies_unigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mham_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspam_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/07/kdp92n316wddp9pz1d_9m9400000gn/T/ipykernel_14409/1412443664.py\u001b[0m in \u001b[0;36mnaive_bayes\u001b[0;34m(ham_count, spam_count, freq, xtest, ngram)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# probability for spam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mp_spam\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalculate_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspam_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# probability for spam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/07/kdp92n316wddp9pz1d_9m9400000gn/T/ipykernel_14409/1129694257.py\u001b[0m in \u001b[0;36mcalculate_probability\u001b[0;34m(test, freq, class_count, is_spam, ngram)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlaplace_smoothing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_spam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/assignment3/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1538\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df = pd.read_csv('emails.csv')\n",
    "\n",
    "    x = df.text.values\n",
    "    y = df.spam.values\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "    ham_count = np.count_nonzero(ytrain == 0)\n",
    "    spam_count = np.count_nonzero(ytrain == 1)\n",
    "\n",
    "    print(\"--------------------------UNIGRAM DEFAULT PARAM----------------------------\")\n",
    "    freq = get_frequencies_unigram(xtrain, ytrain, 1, 1.0)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 1)\n",
    "    classification_report(ytest, ypred)\n",
    "\n",
    "    print(\"\\n-------------------UNIGRAM min_df = 0.01, max_df = 0.8-------------------\")\n",
    "    freq = get_frequencies_unigram(xtrain, ytrain, 0.01, 0.8)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 1)\n",
    "    classification_report(ytest, ypred)\n",
    "\n",
    "    print(\"\\n----------UNIGRAM WITHOUT STOPWORDS min_df = 0.01, max_df = 0.8----------\")\n",
    "    freq = get_frequencies_unigram_without_stopwords(xtrain, ytrain, 0.01, 0.8)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 1)\n",
    "    classification_report(ytest, ypred)\n",
    "\n",
    "    print(\"\\n--------------------------BIGRAM DEFAULT PARAM---------------------------\")\n",
    "    freq = get_frequencies_bigram(xtrain, ytrain, 1, 1.0)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 2)\n",
    "    classification_report(ytest, ypred)\n",
    "\n",
    "    print(\"\\n--------------------BIGRAM min_df = 0.01, max_df = 0.8--------------------\")\n",
    "    freq = get_frequencies_bigram(xtrain, ytrain, 0.01, 0.8)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 2)\n",
    "    classification_report(ytest, ypred)\n",
    "\n",
    "    print(\"\\n----------BIGRAM WITHOUT STOPWORDS min_df = 0.01, max_df = 0.8-----------\")\n",
    "    freq = get_frequencies_bigram_without_stopwords(xtrain, ytrain, 0.01, 0.8)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 2)\n",
    "    classification_report(ytest, ypred)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd1dfeb",
   "metadata": {},
   "source": [
    "In the main function, after reading the e-mails data, train and test data are created. Then, frequencies are calculated with different parameters like unigram, bigram, max_df and min_df. Naive Bayes algorithm is revoked by substituting in the frequencies calculated beforehand. Finally, classification report which includes such as accuracy, recall, precision and f1 score is obtained. The results are shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608100b",
   "metadata": {},
   "source": [
    "When evaluating the performance of a data science model, accuracy is not always the best indicator.\n",
    "Some real-world problems may have a very imbalanced class, and using accuracy may not provide us with enough confidence to understand the algorithm's performance.\n",
    "Spam data accounts for approximately 20% of our data in the email spamming problem that we are attempting to solve. If our algorithm correctly classifies all emails as non-spam, it will achieve an accuracy of 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175d7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
