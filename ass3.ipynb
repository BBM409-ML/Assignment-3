{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "325ae8dc-5972-44fd-9a90-9253a47347d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------UNIGRAM DEFAULT PARAM----------------------------\n",
      "accuracy  : 0.8237347294938918\n",
      "precision : 1.0\n",
      "recall    : 0.23193916349809887\n",
      "f1_score  : 0.3765432098765432\n",
      "\n",
      "-------------------UNIGRAM min_df = 0.01, max_df = 0.8-------------------\n",
      "accuracy  : 0.8656195462478184\n",
      "precision : 0.9658119658119658\n",
      "recall    : 0.4296577946768061\n",
      "f1_score  : 0.5947368421052632\n",
      "\n",
      "----------UNIGRAM WITHOUT STOPWORDS min_df = 0.01, max_df = 0.8----------\n",
      "accuracy  : 0.9685863874345549\n",
      "precision : 0.8873720136518771\n",
      "recall    : 0.9885931558935361\n",
      "f1_score  : 0.9352517985611511\n",
      "\n",
      "--------------------------BIGRAM DEFAULT PARAM---------------------------\n",
      "accuracy  : 0.9066317626527051\n",
      "precision : 0.9936708860759493\n",
      "recall    : 0.596958174904943\n",
      "f1_score  : 0.7458432304038006\n",
      "\n",
      "--------------------BIGRAM min_df = 0.01, max_df = 0.8--------------------\n",
      "accuracy  : 0.705933682373473\n",
      "precision : 0.43833333333333335\n",
      "recall    : 1.0\n",
      "f1_score  : 0.6095017381228273\n",
      "\n",
      "----------BIGRAM WITHOUT STOPWORDS min_df = 0.01, max_df = 0.8-----------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_frequencies_bigram_without_stopwords() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3cfdcef1a6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mclassification_reportt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-3cfdcef1a6b2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n----------BIGRAM WITHOUT STOPWORDS min_df = 0.01, max_df = 0.8-----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_frequencies_bigram_without_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mham_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspam_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mclassification_reportt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_frequencies_bigram_without_stopwords() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def classification_reportt(ytest, ypred):\n",
    "    tp = tn = fp = fn = 0\n",
    "    for i in range(len(ytest)):\n",
    "        if ytest[i] == ypred[i]:\n",
    "            if ytest[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if ypred[i] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(\"accuracy  : \" + str(accuracy))\n",
    "    print(\"precision : \" + str(precision))\n",
    "    print(\"recall    : \" + str(recall))\n",
    "    print(\"f1_score  : \" + str(f1_score))\n",
    "    \n",
    "def laplace_smoothing(freq, alpha, class_count, total_count):\n",
    "    return math.log2((freq + alpha) / (class_count + total_count))    \n",
    "\n",
    "def calculate_probability(test, freq, class_count, is_spam, ngram):\n",
    "    prob = 0\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(ngram, ngram))\n",
    "    y = vectorizer.fit_transform([test])\n",
    "    test = vectorizer.get_feature_names()\n",
    "    \n",
    "    for word in test:\n",
    "        if word in freq:\n",
    "            prob += laplace_smoothing(freq[word][is_spam], 1, class_count, len(freq))\n",
    "            \n",
    "        else:\n",
    "            prob += laplace_smoothing(0, 1, class_count, len(freq))\n",
    "    return prob\n",
    "\n",
    "\n",
    "def naive_bayes(ham_count, spam_count, freq, xtest, ngram):\n",
    "    y_pred = []\n",
    "    for test in xtest:\n",
    "        p_spam = math.log2(spam_count / (spam_count + ham_count))\n",
    "        p_ham = math.log2(ham_count / (spam_count + ham_count))\n",
    "\n",
    "        # probability for spam\n",
    "        p_spam += calculate_probability(test, freq, spam_count, 1, ngram)\n",
    "\n",
    "        # probability for spam\n",
    "        p_ham += calculate_probability(test, freq, ham_count, 0, ngram)\n",
    "\n",
    "        if p_spam > p_ham:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def create_dictionary(freq, words, row, is_spam):\n",
    "    i = 0\n",
    "    for item in row:\n",
    "        if item != 0:\n",
    "            if words[i] in freq:\n",
    "                arr = freq[words[i]]\n",
    "                arr[is_spam] += 1\n",
    "                freq[words[i]] = arr\n",
    "\n",
    "            else:\n",
    "                arr = [0, 0]\n",
    "                arr[is_spam] = 1\n",
    "                freq[words[i]] = arr\n",
    "        i += 1\n",
    "    return freq\n",
    "\n",
    "\n",
    "def get_frequencies(vectorizer, xtrain, ytrain):\n",
    "    y = vectorizer.fit_transform(xtrain)\n",
    "    doc_array = y.toarray()\n",
    "\n",
    "    frequency_matrix = pd.DataFrame(data=doc_array, columns=vectorizer.get_feature_names())\n",
    "\n",
    "    words = list(frequency_matrix.columns.values)\n",
    "    freq = {}\n",
    "\n",
    "    for i, j in frequency_matrix.iterrows():\n",
    "        create_dictionary(freq, words, j.tolist(), ytrain[i])\n",
    "    return freq\n",
    "\n",
    "\n",
    "def get_frequencies_unigram(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "                                 lowercase=True, min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)\n",
    "\n",
    "\n",
    "def get_frequencies_unigram_without_stopwords(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "                                 lowercase=True, stop_words=ENGLISH_STOP_WORDS, min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)\n",
    "\n",
    "\n",
    "def get_frequencies_bigram(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2), token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "                                 lowercase=True, min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)\n",
    "\n",
    "\n",
    "def get_frequencies_bigram_without_stopwords(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2), token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "                                 lowercase=True, stop_words=ENGLISH_STOP_WORDS, min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)\n",
    "\n",
    "\n",
    "def get_frequencies_unigram_bigram(xtrain, ytrain):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2), token_pattern=r\"(?u)\\b\\w+\\b\", lowercase=True)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('emails.csv')\n",
    "\n",
    "    x = df.text.values\n",
    "    y = df.spam.values\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "    ham_count = np.count_nonzero(ytrain == 0)\n",
    "    spam_count = np.count_nonzero(ytrain == 1)\n",
    "    \n",
    "    print(\"--------------------------UNIGRAM DEFAULT PARAM----------------------------\")\n",
    "    freq = get_frequencies_unigram(xtrain, ytrain, 1, 1.0)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 1)\n",
    "    classification_reportt(ytest, ypred)\n",
    "    \n",
    "    print(\"\\n-------------------UNIGRAM min_df = 0.01, max_df = 0.8-------------------\")\n",
    "    freq = get_frequencies_unigram(xtrain, ytrain, 0.01, 0.8)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 1)\n",
    "    classification_reportt(ytest, ypred)\n",
    "    \n",
    "    print(\"\\n----------UNIGRAM WITHOUT STOPWORDS min_df = 0.01, max_df = 0.8----------\")\n",
    "    freq = get_frequencies_unigram_without_stopwords(xtrain, ytrain, 0.01, 0.8)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 1)\n",
    "    classification_reportt(ytest, ypred)\n",
    "    \n",
    "    print(\"\\n--------------------------BIGRAM DEFAULT PARAM---------------------------\")\n",
    "    freq = get_frequencies_bigram(xtrain, ytrain, 1, 1.0)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 2)\n",
    "    classification_reportt(ytest, ypred)\n",
    "    \n",
    "    print(\"\\n--------------------BIGRAM min_df = 0.01, max_df = 0.8--------------------\")\n",
    "    freq = get_frequencies_bigram(xtrain, ytrain, 0.01, 0.8)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 2)\n",
    "    classification_reportt(ytest, ypred)\n",
    "    \n",
    "    print(\"\\n----------BIGRAM WITHOUT STOPWORDS min_df = 0.01, max_df = 0.8-----------\")\n",
    "    freq = get_frequencies_bigram_without_stopwords(xtrain, ytrain, 0.01, 0.8)\n",
    "    ypred = naive_bayes(ham_count, spam_count, freq, xtest, 2)\n",
    "    classification_reportt(ytest, ypred)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9175e-37f9-494e-8545-5d4dcbf5dc44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
