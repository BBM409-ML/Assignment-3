{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd29eaa7",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 3: DETECTION OF SPAM MAILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8281d",
   "metadata": {},
   "source": [
    "In this assignment, it is asked to implement naive bayes algorithm from the scratch and to apply this algorithm for given ham/spam email dataset. Our aim is to predict whether an e-mail is a spam or not using the naive bayes model. This report shows steps we have followed, code implementations and analysis of results for the naive bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f65d68",
   "metadata": {},
   "source": [
    "A Naive Bayes classifier is a probabilistic machine learning model thatâ€™s used for classification task. The crux of the classifier is based on the Bayes theorem. The Naive Bayes algorithm is quick and simple to implement, but the necessity that predictors be independent is its major drawback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ffaa6",
   "metadata": {},
   "source": [
    "## PART 1: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493d8a2",
   "metadata": {},
   "source": [
    "Machine learning algorithms classify data using statistical models. In the instance of spam detection, a trained machine learning model must be able to tell if the sequence of words in an email is more similar to spam emails or not. Different machine learning methods can detect spam, in this assignment we use Naive Bayes. The \"Bayes' theorem,\" which describes the likelihood of an event dependent on prior knowledge, is the foundation of Naive Bayes. The problem is that our characteristics aren't always self-contained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d2a23",
   "metadata": {},
   "source": [
    "Naive Bayes, like other machine learning algorithms, does not understand linguistic context and instead relies on statistical relationships between words to determine whether a piece of text belongs to a specific class. This means that a Naive Bayes spam detector, for example, can be fooled into missing a spam email if the sender simply adds some non-spam words at the end of the message or replaces spammy terms with closely related words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aabada",
   "metadata": {},
   "source": [
    "Consider the words \"not\" and \"interesting\", in this situation, depending on where they appear in the message, the meaning can be radically different. Despite the fact that feature independence in text data is difficult to achieve, the Naive Bayes classifier has proven to be effective in natural language processing applications when correctly configured. In the following sections, it is going to be tested and interpreted whether Naive Bayes is succesful on email spam detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27432dc4",
   "metadata": {},
   "source": [
    "On the other hand, many spam e-mails contain a large amount of \"spammy\" terms such as \"free,\" \"money,\" \"product,\" and so on. This knowledge may aid us in making better decisions when it comes to the design of the spam detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c838d5",
   "metadata": {},
   "source": [
    "Therefore, detecting e-mails by analizing its words is a feasible approach to solve this problem. Generally, manipulative, needy, cheap, shady and fat-fetched words such as \"exclusive deal\", \"free, \"limited time\", \"offer expires\", \"urgent\", \"best price\", \"bargain\", \"great offer\", \"money\" are well-known as spammy. On the other hand, classifying e-mails based on only words can be problematic due to the deficit of context interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311428c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24abba36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subject: great nnews  hello , welcome to medzo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subject: here ' s a hot play in motion  homela...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Subject: undeliverable : home based business f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1\n",
       "5  Subject: great nnews  hello , welcome to medzo...     1\n",
       "6  Subject: here ' s a hot play in motion  homela...     1\n",
       "7  Subject: save your money buy getting this thin...     1\n",
       "8  Subject: undeliverable : home based business f...     1\n",
       "9  Subject: save your money buy getting this thin...     1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"emails.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e45b166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (5728, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3632a627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4360\n",
       "1    1368\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"spam\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97cf07c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='spam', ylabel='count'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x=\"spam\", data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427351d",
   "metadata": {},
   "source": [
    "As can be seen above, the given data has two columns which are \"text\" and \"spam\". Spam=1 means that this e-mail is actually spam. Thus, our data has 4360 ham and 1368 spam mails which makes total 5728 e-mails. It can be seen that there is an imbalance on the number of ham and spam e-mails in the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5984757",
   "metadata": {},
   "source": [
    "In order to be able to interpret the data, it is needed to compute the frequencies of words and create a dictionary which holds unique words in the given data along with their frequencies. After reading the data file and storing it into a pandas dataframe, the data is splitted into train and test with shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154566d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(vectorizer, xtrain, ytrain):\n",
    "    y = vectorizer.fit_transform(xtrain)\n",
    "    doc_array = y.toarray()\n",
    "\n",
    "    frequency_matrix = pd.DataFrame(data=doc_array, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    words = list(frequency_matrix.columns.values)\n",
    "    freq = {}\n",
    "    words_count = [0,0]\n",
    "    \n",
    "    for i, j in frequency_matrix.iterrows():\n",
    "        create_dictionary(freq, words, j.tolist(), ytrain[i], words_count)\n",
    "    return freq, frequency_matrix, words_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97acefa",
   "metadata": {},
   "source": [
    "In this function, the vectorizer is supplied with the training data and frequency matrix is created. After that each row in frequency matrix is sent to \"create_dictionary\" function to calculate frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921fbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(freq, words, row, is_spam, words_count):\n",
    "    i = 0\n",
    "    for item in row:\n",
    "        if item != 0:\n",
    "            if words[i] in freq:\n",
    "                arr = freq[words[i]]\n",
    "                arr[is_spam] += item\n",
    "                freq[words[i]] = arr\n",
    "                words_count[is_spam] += item\n",
    "\n",
    "            else:\n",
    "                arr = [0, 0]\n",
    "                arr[is_spam] = item\n",
    "                freq[words[i]] = arr\n",
    "                words_count[is_spam] += item\n",
    "        i += 1\n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8041238",
   "metadata": {},
   "source": [
    "This function returns a dictioanry which includes each unique word as key and its frequencies on spam and ham mails as a list in the value. For instance, 'subject', [3498, 1084] means that the word \"subject\" is mentioned in 3498 ham mails and 1084 spam mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f7f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_unigram(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), token_pattern=r'\\b[a-zA-Z]\\w+\\b',\n",
    "                                 max_df=max_df, min_df=min_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac16bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_unigram_without_stopwords(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), token_pattern=r'\\b[a-zA-Z]\\w+\\b',\n",
    "                                 stop_words=ENGLISH_STOP_WORDS)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d7c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_bigram(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2), token_pattern=r'\\b[a-zA-Z]\\w+\\b',\n",
    "                                 min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc28596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_bigram_without_stopwords(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2), token_pattern=r'\\b[a-zA-Z]\\w+\\b',\n",
    "                                 stop_words=ENGLISH_STOP_WORDS, min_df=min_df, max_df=max_df)\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b45e0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies_unigram_bigram(xtrain, ytrain, min_df, max_df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    return get_frequencies(vectorizer, xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186195d",
   "metadata": {},
   "source": [
    "In order to implement BoW model, CountVectorizer is used. The functions above create different vectorizers according to whether words are tokenized as unigram or bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9123c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "df = pd.read_csv('emails.csv')\n",
    "x = df.text.values\n",
    "y = df.spam.values\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baab6b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------UNIGRAM MOST FREQUENT WORDS----------------------------\n",
      "[('the', [33277, 7604]), ('to', [27024, 6864]), ('and', [16837, 5448])]\n",
      "-----------------UNIGRAM (WITHOUT STOPWORDS) MOST FREQUENT WORDS-----------------\n",
      "[('enron', [10785, 0]), ('ect', [9150, 1]), ('vince', [6929, 0])]\n"
     ]
    }
   ],
   "source": [
    "unigram_freq, freq_matrix, words_count = get_frequencies_unigram(xtrain, ytrain, 1, 1.0)\n",
    "sorted_unigram = sorted(unigram_freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "unigram_freq_no_stopwords, freq_matrix_no_stopwords, words_count = get_frequencies_unigram_without_stopwords(xtrain, ytrain, 1, 1.0)\n",
    "sorted_unigram_no_stopwords = sorted(unigram_freq_no_stopwords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"--------------------------UNIGRAM MOST FREQUENT WORDS----------------------------\")\n",
    "print(list(sorted_unigram)[:3])\n",
    "print(\"-----------------UNIGRAM (WITHOUT STOPWORDS) MOST FREQUENT WORDS-----------------\")\n",
    "print(list(sorted_unigram_no_stopwords)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad7e83",
   "metadata": {},
   "source": [
    "It can be seen that the word \"subject\" is the most frequent word in both ham and spam mails but it does not help our algorithm to classify better since it is not rare and cannot distinguish spam e-mails from ham e-mails. When we include stopwords, \"to\" and \"the\" are the next most frequent words in both spam and ham e-mails. However, they don't contribute any useful information neither, since they are frequently written in both ham and spam e-mails. On the other hand, when stopwords are removed, the most frequent words (in ham e-mails) after \"subject\" are \"vince\" and \"enron\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00ff4bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaenerfax</th>\n",
       "      <th>aal</th>\n",
       "      <th>aaldous</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aall</th>\n",
       "      <th>aanalysis</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aawesome</th>\n",
       "      <th>...</th>\n",
       "      <th>zwwyw</th>\n",
       "      <th>zwzm</th>\n",
       "      <th>zxghlajf</th>\n",
       "      <th>zyban</th>\n",
       "      <th>zyc</th>\n",
       "      <th>zygoma</th>\n",
       "      <th>zymg</th>\n",
       "      <th>zzmacmac</th>\n",
       "      <th>zzncacst</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 30138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaaenerfax  aal  aaldous  aaliyah  aall  aanalysis  aaron  \\\n",
       "0   0    0           0    0        0        0     0          0      0   \n",
       "1   0    0           0    0        0        0     0          0      0   \n",
       "2   0    0           0    0        0        0     0          0      0   \n",
       "3   0    0           0    0        0        0     0          0      0   \n",
       "4   0    0           0    0        0        0     0          0      0   \n",
       "5   0    0           0    0        0        0     0          0      0   \n",
       "6   0    0           0    0        0        0     0          0      0   \n",
       "7   0    0           0    0        0        0     0          0      0   \n",
       "8   0    0           0    0        0        0     0          0      0   \n",
       "9   0    0           0    0        0        0     0          0      0   \n",
       "\n",
       "   aawesome  ...  zwwyw  zwzm  zxghlajf  zyban  zyc  zygoma  zymg  zzmacmac  \\\n",
       "0         0  ...      0     0         0      0    0       0     0         0   \n",
       "1         0  ...      0     0         0      0    0       0     0         0   \n",
       "2         0  ...      0     0         0      0    0       0     0         0   \n",
       "3         0  ...      0     0         0      0    0       0     0         0   \n",
       "4         0  ...      0     0         0      0    0       0     0         0   \n",
       "5         0  ...      0     0         0      0    0       0     0         0   \n",
       "6         0  ...      0     0         0      0    0       0     0         0   \n",
       "7         0  ...      0     0         0      0    0       0     0         0   \n",
       "8         0  ...      0     0         0      0    0       0     0         0   \n",
       "9         0  ...      0     0         0      0    0       0     0         0   \n",
       "\n",
       "   zzncacst  zzzz  \n",
       "0         0     0  \n",
       "1         0     0  \n",
       "2         0     0  \n",
       "3         0     0  \n",
       "4         0     0  \n",
       "5         0     0  \n",
       "6         0     0  \n",
       "7         0     0  \n",
       "8         0     0  \n",
       "9         0     0  \n",
       "\n",
       "[10 rows x 30138 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_matrix.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5fe29a",
   "metadata": {},
   "source": [
    "The frequency matrices for unigram are shown above. The first matrix has 30259 columns which means there are 30259 unique words, each row shows the frequencies of each word on that particular e-mail. For instance, in the first e-mail of our data, the word \"aa\" is not typed because its value is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc01b68",
   "metadata": {},
   "source": [
    "## PART 2: Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af450b58",
   "metadata": {},
   "source": [
    "When tranining the machine learning model on the training data set, each term is assigned a weight based on how many times it appears in spam and ham emails. For example, if \"win big money prize\" is one of the features and only appears in spam emails, it will be assigned a higher probability of being spam. If \"important meeting\" is only mentioned in ham emails, then including it in an email increases the likelihood of that email being classified as not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d663c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(ham_count, spam_count, freq, xtest, ngram, words_count, stop_words):\n",
    "    y_pred = []\n",
    "    for test in xtest:\n",
    "        p_spam = math.log2(spam_count / (spam_count + ham_count))\n",
    "        p_ham = math.log2(ham_count / (spam_count + ham_count))\n",
    "\n",
    "        # probability for spam\n",
    "        p_spam += calculate_probability(test, freq, 1, words_count[1], ngram, stop_words)\n",
    "\n",
    "        # probability for spam\n",
    "        p_ham += calculate_probability(test, freq, 0, words_count[0], ngram, stop_words)\n",
    "\n",
    "        if p_spam > p_ham:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef3561",
   "metadata": {},
   "source": [
    "In this function, the probability of being spam or ham are calculated respectively. Then, e-mail is labeled as spam if the probability calculated for spam is greater than probability calculated for ham, and vice versa. One thing important here to state is that the probabilities are calculated with logairthm function in order to prevent numerical underflow when calculating multiplicative probabilities. The probabilities are substitued into logarithm function, then they are summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e642b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(test, freq, is_spam, total_word_count, ngram, stop_words):\n",
    "    prob = 0\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(ngram, ngram), stop_words=stop_words)\n",
    "    y = vectorizer.fit_transform([test])\n",
    "    test = vectorizer.get_feature_names()\n",
    "   \n",
    "    for word in test:\n",
    "        if word in freq:\n",
    "            prob += laplace_smoothing(freq[word][is_spam], 1, total_word_count, len(freq))\n",
    "\n",
    "        else:\n",
    "            prob += laplace_smoothing(0, 1, total_word_count, len(freq))\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94510421",
   "metadata": {},
   "source": [
    "This function calls laplace smoothing for each word in test sample, sums results up, returns the total probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92fe90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(freq, alpha, class_count, total_count):\n",
    "    return math.log2((freq + alpha) / (class_count + total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393b45f",
   "metadata": {},
   "source": [
    "When calculating the probabilites, we may encounter some words that we havent during training. In this case, the frequency of that word would be 0, which dominates the result of our probability. This problem can be eliminated by using Laplace Smoothing also known as Add One Smoothing when alpha is 1. In this technique, we add 1 to the numerator, and we add k to the denominator. So, in the case that we donâ€™t have a particular ingredient in our training set, the posterior probability comes out to 1 / N + k instead of zero. Plugging in this value into the product does not destroy our ability to make a prediction in the same way that plugging in a zero does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9b2d6",
   "metadata": {},
   "source": [
    "In \"laplace_smoothing\" function, freq is the value of how many times the current word is typed in spam/ham e-mails, class_count is total spam/ham e-mail count and total_count is the number of total unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a95b3",
   "metadata": {},
   "source": [
    "EÄžER FONKSÄ°YON DEÄžÄ°ÅžÄ°RSE BU KISMI DA DEÄžÄ°ÅžTÄ°RELÄ°M!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3369fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(ytest, ypred):\n",
    "    tp = tn = fp = fn = 0\n",
    "    for i in range(len(ytest)):\n",
    "        if ytest[i] == ypred[i]:\n",
    "            if ytest[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if ypred[i] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(\"accuracy  : \" + str(accuracy))\n",
    "    print(\"precision : \" + str(precision))\n",
    "    print(\"recall    : \" + str(recall))\n",
    "    print(\"f1_score  : \" + str(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a46dc",
   "metadata": {},
   "source": [
    "In this function, predicted classification and actual classification are compared. Accuracy, recall, precision and f1 score are calculated by computing confusion matrix values like FN, FP, TP and TN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ba223",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PART 3: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7986f95",
   "metadata": {},
   "source": [
    "### Analyzing effect of the words on prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9530e",
   "metadata": {},
   "source": [
    "In the second part of the assignment, term frequency is used when implementing Naive Bayes, which means that we consider entire dataset as equally important and and made our computations solely based on how many times a word occured in the dataset. However, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f1933",
   "metadata": {},
   "source": [
    "When calculating TF, all terms are given equal weight. However, it is well known that some phrases, such as \"is,\" \"of,\" and \"that,\" may appear frequently but have little meaning. As a result, we must scale down the frequent phrases while scaling up the rare ones. In this part of the assignment, TF-IDF (Inverse Document Frequency) will be implemented and compared to previous frequency implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d2611",
   "metadata": {},
   "source": [
    "When computing IDF, an inverted document frequency factor is used, which reduces the weight of terms that appear frequently in the document collection while increasing the weight of terms that occur infrequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d834e",
   "metadata": {},
   "source": [
    "The information density factor (IDF) is the inverse of the document frequency, which assesses the informativeness of word t. When we compute IDF, it will be very low for the most often occurring words, such as stop words (since stop words like \"is\" appear in practically every text, and N/df will give that word a very low number). Finally, we have what we're looking for: a relative weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc25105",
   "metadata": {},
   "source": [
    "Thus, it can be said that TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents and we will make use of that in order to improve our classification performance by selecting the most effective words in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb1e9760-1eb7-47f1-90d2-fbafb7199f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_effective_words(data):\n",
    "    #instantiate CountVectorizer() \n",
    "    cv=CountVectorizer(analyzer='word', token_pattern=r'\\b[a-zA-Z]\\w+\\b',) \n",
    "\n",
    "    # this steps generates word counts for the words in your docs \n",
    "    word_count_vector=cv.fit_transform(data)\n",
    "\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_weights\"]) \n",
    "    result = df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "    # sort ascending \n",
    "    return result[-9:], result[:9]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9175d7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------SPAM MOST EFFECTIVE WORDS-----------------------\n",
      "           idf_weights\n",
      "nominal       7.528689\n",
      "nominate      7.528689\n",
      "distent       7.528689\n",
      "nomme         7.528689\n",
      "nomore        7.528689\n",
      "nomorel       7.528689\n",
      "distant       7.528689\n",
      "nomatter      7.528689\n",
      "kuhvzhfdm     7.528689\n",
      "------------------SPAM LEAST EFFECTIVE WORDS-----------------------\n",
      "         idf_weights\n",
      "subject     1.000000\n",
      "to          1.163938\n",
      "the         1.233423\n",
      "and         1.325141\n",
      "you         1.331227\n",
      "of          1.341452\n",
      "your        1.360124\n",
      "for         1.411693\n",
      "is          1.443051\n",
      "-------------------HAM MOST EFFECTIVE WORDS-----------------------\n",
      "           idf_weights\n",
      "gapping       8.687309\n",
      "gammas        8.687309\n",
      "gammar        8.687309\n",
      "gammal        8.687309\n",
      "quesions      8.687309\n",
      "quesitons     8.687309\n",
      "quesnel       8.687309\n",
      "queensl       8.687309\n",
      "levesque      8.687309\n",
      "-------------------HAM LEAST EFFECTIVE WORDS-----------------------\n",
      "         idf_weights\n",
      "subject     1.000000\n",
      "to          1.072258\n",
      "the         1.086407\n",
      "and         1.200416\n",
      "for         1.225382\n",
      "you         1.226532\n",
      "of          1.270931\n",
      "on          1.300219\n",
      "in          1.304874\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "spam_data = df[df[\"spam\"] == 1][\"text\"].tolist()\n",
    "ham_data = df[df[\"spam\"] == 0][\"text\"].tolist()\n",
    "\n",
    "spam_most_effective, spam_least_effective = list_most_effective_words(spam_data)\n",
    "ham_most_effective, ham_least_effective = list_most_effective_words(ham_data)\n",
    "\n",
    "print(\"-------------------SPAM MOST EFFECTIVE WORDS-----------------------\")\n",
    "print(spam_most_effective)\n",
    "print(\"------------------SPAM LEAST EFFECTIVE WORDS-----------------------\")\n",
    "print(spam_least_effective)\n",
    "print(\"-------------------HAM MOST EFFECTIVE WORDS-----------------------\")\n",
    "print(ham_most_effective)\n",
    "print(\"-------------------HAM LEAST EFFECTIVE WORDS-----------------------\")\n",
    "print(ham_least_effective)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6453e",
   "metadata": {},
   "source": [
    "As mentioned previously, TF-IDF detects the most effective and rare words in our dataset. When the value related to a word is low (like 1.0 in our case), it means that this word does not provide deterministic information to our algoirthm. On the other hand, while the value rises up, it means that this word is effective when classifying as it is rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be88309",
   "metadata": {},
   "source": [
    "For instance, top 10 words with the greatest TF-IDF values for spam classification are listed above as: nominal, nominate, distent, nomme, nomore, nomorel, distant, nomatter with values almost 7.53. The presence of these words most strongly predicts that the e-mail is spam. On the other hand, top 10 words with the lowest TF-IDF values are subject, to, the, and, you, of, your, for and is. It is obvious that most of these words are stopwords and occurs in the majority of the given dataset. For instance, the TF-IDF value of subject is 1.0 which means that this word is typed in all of the spam e-mails and it is the most common  word in our dataset. The absence of these words most strongly predicts that e-mail is spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c7c49",
   "metadata": {},
   "source": [
    "On the other hand, for the ham classification, top 10 words with the greatest TF-IDF values are gapping, gammas, gammar, gammal, quesions, quesitons, quesnel, queensl and levesque with the value almost 8.69. The presence of these words most strongly predicts that the e-mail is ham. On the contary, top 10 words with the lowest TF-IDF values are subject, to, the, and, for, you, of, on, in. These words are also mostly stopwords and the absence of these words most strongly predicts that e-mail is ham."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d0f0f",
   "metadata": {},
   "source": [
    "REIMPLEMENT FOR NAIVE BAYES!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ad10f",
   "metadata": {},
   "source": [
    "###Â Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ed62a",
   "metadata": {},
   "source": [
    "Before supplying our data to CountVectorizer it is required to preprocess the data. First of all, lowering the case of e-mails is essential since the words \"HELLO\" and \"hello\" are the same and lowering them is very helpful for reducing the dimensions by shrinking the size of the dictionary. Whitespaces are removed since they do not have any impact on our classifier. Lastly, numbers are discarded since they do not provide any difference to make our algorithm works more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1abd3f2",
   "metadata": {},
   "source": [
    "###Â Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783db96",
   "metadata": {},
   "source": [
    "The definition of a stop word varies. A stop word is a word that appears frequently in a corpus. Or, given a context, you can consider any word that is devoid of true meaning. Articles and some verbs are commonly regarded as stop words because they do not assist us in determining the context or true meaning of a sentence. These are words that can be removed without affecting the final model that you are training. However, the removal of stopwords is not necessary in all situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f88b1",
   "metadata": {},
   "source": [
    "Stopwords should be removed if they add no new information to your problem. They are usually unnecessary in classification problems because it is possible to discuss the general idea of a text even if stop words are removed. Thus, we remove the low-level information from our text by removing these words, allowing us to focus on the important information. In other words, the removal of such words has no negative consequences for the model we train for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89c6d8",
   "metadata": {},
   "source": [
    "Removal of stop words definitely reduces the dataset size and thus reduces the training time due to the fewer number of tokens involved in the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb459a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In e-mail classification, the occurence of stopwords are not essential because the context of the mail, whether it is spam or ham, can be inferred from the other words. Furthermore, removal of stopwords shrinks the data and improves the performance of our algorithm. The analysis of the removal of stopwords wil be discussed in the upcoming section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa3241",
   "metadata": {},
   "source": [
    "### Tokenizing - Unigram, Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be1170",
   "metadata": {},
   "source": [
    "Text data must be \"tokenized\" before being fed to machine learning algorithms, both for training and for making predictions on new data. Tokenization is the process of dividing text data into smaller pieces. If you divide the preceding data set into single words (also known as unigrams), you'll get the following vocabulary. It's worth noting that I only used each word once. If we use occurence of a single word, it is called unigram. On the other hand, if we use the occurence of two adjacent words, it is called bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb56a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "When creating vectorizers, we set different max_df and min_df values. \"max_df\" is used for removing terms that appear too frequently. \"max_df\" is set to 1.0 by default, which indicates \"exclude terms that exist in more than 100% of the documents.\" As a result, no terms are ignored by the default configuration. On the other hand, \"min_df\" is used for removing terms that appear too infrequently. The default value for min df is 1, which implies \"ignore terms appearing in fewer than one document.\" As a result, no terms are ignored by the default configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cff323",
   "metadata": {},
   "source": [
    "If we use the default settings, which include all phrases, you will generate far more tokens. As a result, our classification process will take longer. Removing too rare and too common terms may be useful. However, the most significant issue is that if the min and max are set incorrectly, some vital terms may be lost, lowering the quality. In this assignment, we prefer to discard the most rare and common words since we believe that they do not have a significant effect on our classifier quality. We set min_df as 0.01 and max_df as 0.8, which means that our classifier ignores the words included in more than 80% of e-mails and included in less than 1% of e-mails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e8eb8",
   "metadata": {},
   "source": [
    "To summarize, trimming terms using min_df and max_df improves the performance of our classifier rather than the quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685fa52",
   "metadata": {},
   "source": [
    "MAX_DF VE MIN_DF DEÄžÄ°ÅžÄ°RSE BURAYI TEKRAR DÃœZENLE!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47057d57",
   "metadata": {},
   "source": [
    "## PART 4: Calculation of Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36ef645c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------UNIGRAM DEFAULT PARAM----------------------------\n",
      "accuracy  : 0.9738219895287958\n",
      "precision : 0.903114186851211\n",
      "recall    : 0.9923954372623575\n",
      "f1_score  : 0.9456521739130435\n",
      "\n",
      "-------------------------UNIGRAM WITHOUT STOPWORDS-------------------------\n",
      "accuracy  : 0.9790575916230366\n",
      "precision : 0.9222614840989399\n",
      "recall    : 0.9923954372623575\n",
      "f1_score  : 0.9560439560439561\n",
      "\n",
      "----------------------------BIGRAM DEFAULT PARAM----------------------------\n",
      "accuracy  : 0.9537521815008726\n",
      "precision : 0.8322784810126582\n",
      "recall    : 1.0\n",
      "f1_score  : 0.9084628670120898\n",
      "\n",
      "--------------------------BIGRAM WITHOUT STOPWORDS---------------------------\n",
      "accuracy  : 0.8228621291448517\n",
      "precision : 0.5643776824034334\n",
      "recall    : 1.0\n",
      "f1_score  : 0.7215363511659808\n",
      "\n",
      "-----------------------BIGRAM & UNIGRAM DEFAULT PARAM-------------------------\n",
      "accuracy  : 0.912739965095986\n",
      "precision : 0.7245179063360881\n",
      "recall    : 1.0\n",
      "f1_score  : 0.8402555910543131\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('emails.csv')\n",
    "\n",
    "x = df.text.values\n",
    "y = df.spam.values\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "ham_count = np.count_nonzero(ytrain == 0)\n",
    "spam_count = np.count_nonzero(ytrain == 1)\n",
    "\n",
    "print(\"--------------------------UNIGRAM DEFAULT PARAM----------------------------\")\n",
    "freq, matrix, words_count = get_frequencies_unigram(xtrain, ytrain, 1, 1.0)\n",
    "ypred = naive_bayes(ham_count, spam_count, freq, xtest, 1, words_count, None)\n",
    "classification_report(ytest, ypred)\n",
    "\n",
    "print(\"\\n-------------------------UNIGRAM WITHOUT STOPWORDS-------------------------\")\n",
    "freq, matrix, words_count = get_frequencies_unigram_without_stopwords(xtrain, ytrain, 1, 1.0)\n",
    "ypred = naive_bayes(ham_count, spam_count, freq, xtest, 1, words_count, ENGLISH_STOP_WORDS)\n",
    "classification_report(ytest, ypred)\n",
    "\n",
    "print(\"\\n----------------------------BIGRAM DEFAULT PARAM----------------------------\")\n",
    "freq, matrix, words_count = get_frequencies_bigram(xtrain, ytrain, 1, 1.0)\n",
    "ypred = naive_bayes(ham_count, spam_count, freq, xtest, 2, words_count, None)\n",
    "classification_report(ytest, ypred)\n",
    "\n",
    "print(\"\\n--------------------------BIGRAM WITHOUT STOPWORDS---------------------------\")\n",
    "freq, matrix, words_count = get_frequencies_bigram_without_stopwords(xtrain, ytrain, 1, 1.0)\n",
    "ypred = naive_bayes(ham_count, spam_count, freq, xtest, 2, words_count, ENGLISH_STOP_WORDS)\n",
    "classification_report(ytest, ypred)\n",
    "\n",
    "print(\"\\n-----------------------BIGRAM & UNIGRAM DEFAULT PARAM-------------------------\")\n",
    "freq, matrix, words_count = get_frequencies_unigram_bigram(xtrain, ytrain, 1, 1.0)\n",
    "ypred = naive_bayes(ham_count, spam_count, freq, xtest, 2, words_count, None)\n",
    "classification_report(ytest, ypred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd1dfeb",
   "metadata": {},
   "source": [
    "In the main function, after reading the e-mails data, train and test data are created. Then, frequencies are calculated with different parameters like unigram, bigram, max_df and min_df. Naive Bayes algorithm is revoked by substituting in the frequencies calculated beforehand. Finally, classification report which includes accuracy, recall, precision and f1 score is obtained. The results are shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48ae30",
   "metadata": {},
   "source": [
    "Moreover, it can be seen from the results that when the stopwords are removed, the size of the dictionary is decreased, therefore the computation time is also decreased. On the contrary, no remarkable change is observed on the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608100b",
   "metadata": {},
   "source": [
    "When evaluating the performance of a data science model, accuracy is not always the best indicator.\n",
    "Some real-world problems may have a very imbalanced class, and using accuracy may not provide us with enough confidence to understand the algorithm's performance.\n",
    "Spam data accounts for approximately 20% of our data in the email spamming problem that we are attempting to solve. If our algorithm correctly classifies all emails as non-spam, it will achieve an accuracy of 80%. So, we obtained all confusion matrix values which are accuracy, recall, precision and f1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747fefa",
   "metadata": {},
   "source": [
    "From the results obtained above, we can say that our classifier works quite well since the results obtained are generally high. For unigram without stopwords, accuracy=0.97, precision=0.90, recall=0.99 and f1_score=0.95. When we remove the stopwords, the results are slightly changed, also since a relatively small dataset is used, the computation time is decreased as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e259de",
   "metadata": {},
   "source": [
    "For bigram, the results are also gratifying. When stopwords included, accuracy=0.95 , precision=0.83 , recall=1.0 , f1_score=0.91 . In our opinion, providing stopwords parameter to bigram is not a wise choice because bigram tokenizes words as pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a4b1b",
   "metadata": {},
   "source": [
    "Lastly, when we both provide unigram and bigram parameters, the dataset increases but there is no notable increase on the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd22ce4",
   "metadata": {},
   "source": [
    "TEKRAR Ã‡ALIÅžTIRIRSAK DEÄžERLERÄ° DÃœZENLEYELÄ°M !!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
